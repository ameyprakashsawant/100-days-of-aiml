# Project 2: PCA From Scratch

Implementing Principal Component Analysis using pure linear algebra.

## ðŸ“š Concepts Used

- Eigenvalues & Eigenvectors
- Covariance Matrix
- SVD (Singular Value Decomposition)
- Matrix Multiplication
- Variance

## ðŸŽ¯ Project Goals

1. Implement PCA from scratch (no sklearn)
2. Visualize dimensionality reduction
3. Compare with sklearn's PCA
4. Apply to real dataset

## ðŸš€ How to Run

```bash
pip install numpy matplotlib
python pca_from_scratch.py
```

## ðŸ“– Theory

### PCA Algorithm Steps:

1. **Center the data**: XÌ„ = X - mean(X)
2. **Compute covariance matrix**: C = (1/n) XÌ„áµ€XÌ„
3. **Compute eigenvectors/values**: Cv = Î»v
4. **Sort by eigenvalue** (descending)
5. **Project**: X_reduced = XÌ„ @ V[:, :k]

### Alternative: SVD Approach

1. Center data: XÌ„
2. Compute SVD: XÌ„ = UÎ£Váµ€
3. Principal components are columns of V
4. Singular valuesÂ² / n = Eigenvalues

### Explained Variance

$$\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_j \lambda_j}$$
